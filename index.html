<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Vision & Session Recorder</title>
    <!-- Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Configure Tailwind -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        sans: ['Inter', 'sans-serif'],
                    },
                }
            }
        }
    </script>
    <style>
        /* Custom styles for better mobile experience and layout */
        body {
            background-color: #0d1117;
            color: #c9d1d9;
            font-family: 'Inter', sans-serif;
            min-height: 100vh;
        }

        .container-app {
            max-width: 100vw;
            margin: 0 auto;
            padding: 1rem;
        }

        .video-container {
            position: relative;
            width: 100%;
            /* 16:9 aspect ratio for video/canvas */
            padding-bottom: 56.25%; 
            background: #21262d;
            border-radius: 0.5rem;
            overflow: hidden;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.3), 0 4px 6px -2px rgba(0, 0, 0, 0.15);
        }

        video, canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover;
            transform: scaleX(-1); /* Flip horizontally for mirror effect (webcam) */
        }

        /* Canvas should overlay video perfectly */
        #predictionCanvas {
            z-index: 10;
            pointer-events: none; /* Allows clicks to fall through to buttons if needed */
        }

        .control-btn {
            @apply px-4 py-2 font-semibold text-white transition duration-200 ease-in-out rounded-xl shadow-lg transform hover:scale-[1.02] active:scale-[0.98];
        }

        #logPanel {
            max-height: 300px;
            overflow-y: auto;
            background-color: #161b22;
            border: 1px solid #30363d;
        }

        /* Mobile specific styles */
        @media (min-width: 768px) {
            .container-app {
                max-width: 1200px;
                display: grid;
                grid-template-columns: 2fr 1fr;
                gap: 2rem;
            }
        }
    </style>
    <!-- Load Firebase & ML Libraries -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
    <script src='https://cdn.jsdelivr.net/npm/tesseract.js@5/dist/tesseract.min.js'></script>
    <script type="module">
        // Import Firebase modules
        import { initializeApp } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-app.js";
        import { getAuth, signInAnonymously, signInWithCustomToken, onAuthStateChanged } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-auth.js";
        import { getFirestore, doc, addDoc, setDoc, onSnapshot, collection, query, serverTimestamp, setLogLevel } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-firestore.js";

        // Global state variables
        let app, db, auth;
        let userId = null;
        let isAuthReady = false;

        // ML/Camera State
        let model = null;
        let video = null;
        let canvas = null;
        let ctx = null;
        let animationFrameId = null;
        let sessionData = null; // { startTime, endTime, detections: [] }

        // --- Configuration & Initialization ---

        // Set Firebase Log Level to Debug for development visibility
        setLogLevel('Debug');

        // Global Firebase configuration variables (MANDATORY USE)
        const appId = typeof __app_id !== 'undefined' ? __app_id : 'default-app-id';
        let firebaseConfig;
        try {
            firebaseConfig = JSON.parse(typeof __firebase_config !== 'undefined' ? __firebase_config : '{}');
        } catch (e) {
            console.error("Failed to parse Firebase config:", e);
            firebaseConfig = {};
        }
        const initialAuthToken = typeof __initial_auth_token !== 'undefined' ? __initial_auth_token : null;
        const SESSIONS_COLLECTION = 'sessions'; // Subcollection name

        /**
         * Initializes Firebase and authenticates the user.
         */
        async function initializeFirebase() {
            try {
                if (Object.keys(firebaseConfig).length === 0) {
                    logMessage('Error: Firebase configuration is missing or invalid.', 'error');
                    return;
                }

                // 1. Initialize App
                app = initializeApp(firebaseConfig);
                db = getFirestore(app);
                auth = getAuth(app);

                // 2. Set up Auth State Listener
                onAuthStateChanged(auth, async (user) => {
                    if (user) {
                        userId = user.uid;
                        logMessage(`Authenticated: User ID ${userId.substring(0, 8)}...`);
                        isAuthReady = true;
                        document.getElementById('userIdDisplay').textContent = `User ID: ${userId}`;
                        // Start listening to the session history once authenticated
                        setupSessionHistoryListener();
                    } else {
                        userId = null;
                        isAuthReady = true;
                        logMessage('Authentication failed or logged out. Please try again.', 'error');
                    }
                });

                // 3. Sign In
                if (initialAuthToken) {
                    await signInWithCustomToken(auth, initialAuthToken);
                } else {
                    await signInAnonymously(auth);
                }

            } catch (error) {
                logMessage(`Firebase Initialization Error: ${error.message}`, 'error');
                console.error("Firebase Initialization Error:", error);
            }
        }

        /**
         * Sets up the camera stream and loads the ML model.
         */
        async function setupCameraAndML() {
            logMessage('Initializing camera and ML model...');
            video = document.getElementById('webcamVideo');
            canvas = document.getElementById('predictionCanvas');
            ctx = canvas.getContext('2d');
            
            try {
                // Get user media stream (prefers front-facing on mobile if available)
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    video: { facingMode: 'environment' }, // Try back camera first for mobile
                    audio: false 
                });
                video.srcObject = stream;
                // Wait for the video to load metadata to get actual dimensions
                await new Promise(resolve => video.onloadedmetadata = resolve);

                // Set canvas dimensions to match video dimensions
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;

                logMessage('Camera stream started successfully.');

                // Load COCO-SSD model
                model = await cocoSsd.load();
                logMessage('Object Detection Model loaded successfully. Ready to detect.');

                // Start the detection loop
                document.getElementById('startDetectionBtn').disabled = false;
                document.getElementById('stopDetectionBtn').disabled = true;

            } catch (err) {
                logMessage(`Error accessing camera: ${err.name} - ${err.message}. Make sure your browser has permission.`, 'error');
                console.error("Camera Setup Error:", err);
            }
        }

        // --- Object Detection Logic ---

        /**
         * Main detection and drawing loop.
         */
        async function detectFrame() {
            if (!model || video.paused || video.ended) {
                if (animationFrameId) cancelAnimationFrame(animationFrameId);
                return;
            }

            try {
                const predictions = await model.detect(video);
                
                // Clear canvas and redraw
                ctx.clearRect(0, 0, canvas.width, canvas.height);

                if (predictions.length > 0) {
                    // Update current detections list
                    updateDetectionsList(predictions);
                }
                
                predictions.forEach(prediction => {
                    // Draw bounding box
                    const [x, y, width, height] = prediction.bbox;
                    
                    ctx.beginPath();
                    ctx.lineWidth = "4";
                    ctx.strokeStyle = prediction.class === 'person' ? "#ff5722" : "#4CAF50";
                    ctx.rect(x, y, width, height);
                    ctx.stroke();

                    // Draw label background
                    ctx.fillStyle = prediction.class === 'person' ? "#ff5722" : "#4CAF50";
                    const text = `${prediction.class} (${Math.round(prediction.score * 100)}%)`;
                    const textMetrics = ctx.measureText(text);
                    const textHeight = 20; // Approximation
                    
                    ctx.fillRect(x, y - textHeight, textMetrics.width + 10, textHeight);

                    // Draw text label
                    ctx.fillStyle = "#FFFFFF";
                    ctx.font = '16px sans-serif';
                    ctx.fillText(text, x + 5, y - 5);
                });

                // Check if session recording is active and save detection
                if (sessionData) {
                    saveDetectionToSession(predictions);
                }

            } catch (e) {
                console.error("Detection Error:", e);
                logMessage(`Detection failed: ${e.message}`, 'error');
            }

            // Loop the detection process
            animationFrameId = requestAnimationFrame(detectFrame);
        }

        /**
         * Starts the detection process.
         */
        function startDetection() {
            if (model && video.srcObject) {
                logMessage('Real-time object detection started.');
                video.play();
                animationFrameId = requestAnimationFrame(detectFrame);
                document.getElementById('startDetectionBtn').disabled = true;
                document.getElementById('stopDetectionBtn').disabled = false;
            } else {
                logMessage('Cannot start detection. Camera or model is not ready.', 'error');
            }
        }

        /**
         * Stops the detection process.
         */
        function stopDetection() {
            if (animationFrameId) {
                cancelAnimationFrame(animationFrameId);
                animationFrameId = null;
                logMessage('Object detection paused.');
                document.getElementById('startDetectionBtn').disabled = false;
                document.getElementById('stopDetectionBtn').disabled = true;
                // Clear the canvas and detection list
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                document.getElementById('currentDetections').innerHTML = '';
            }
        }

        /**
         * Updates the UI list of currently detected objects.
         */
        function updateDetectionsList(predictions) {
            const list = document.getElementById('currentDetections');
            list.innerHTML = '';
            
            // Group by class and count
            const counts = predictions.reduce((acc, p) => {
                acc[p.class] = (acc[p.class] || 0) + 1;
                return acc;
            }, {});

            Object.keys(counts).forEach(cls => {
                const li = document.createElement('li');
                li.className = 'py-1 text-sm flex justify-between items-center';
                li.innerHTML = `<span class="font-medium">${cls}:</span> <span class="text-green-400">${counts[cls]}</span>`;
                list.appendChild(li);
            });
        }

        // --- OCR Logic (Tesseract.js) ---

        /**
         * Runs OCR on the current video frame.
         */
        async function runOCR() {
            const ocrResultPanel = document.getElementById('ocrResult');
            const ocrBtn = document.getElementById('runOCRBtn');

            if (!video.srcObject) {
                logMessage('Cannot run OCR: Camera is not active.', 'error');
                return;
            }
            
            ocrBtn.textContent = 'Processing...';
            ocrBtn.disabled = true;
            ocrResultPanel.innerHTML = '<p class="text-sm text-yellow-400">Capturing and recognizing text...</p>';

            // 1. Capture the frame onto a temporary canvas
            const tempCanvas = document.createElement('canvas');
            tempCanvas.width = video.videoWidth;
            tempCanvas.height = video.videoHeight;
            const tempCtx = tempCanvas.getContext('2d');
            tempCtx.drawImage(video, 0, 0, tempCanvas.width, tempCanvas.height);
            
            // Get the image data URL
            const imageUrl = tempCanvas.toDataURL('image/png');

            try {
                // 2. Run Tesseract recognition
                const result = await Tesseract.recognize(
                    imageUrl,
                    'eng', // English language model
                    { logger: m => console.log('Tesseract Log:', m) }
                );

                const recognizedText = result.data.text.trim();
                
                // 3. Update UI
                ocrResultPanel.innerHTML = recognizedText 
                    ? `<p class="whitespace-pre-wrap text-sm leading-relaxed">${recognizedText}</p>`
                    : '<p class="text-sm text-red-400">No text recognized in the captured frame.</p>';
                
                logMessage('OCR completed.');

            } catch (e) {
                console.error("OCR Error:", e);
                ocrResultPanel.innerHTML = `<p class="text-sm text-red-400">OCR processing error: ${e.message}</p>`;
                logMessage(`OCR failed: ${e.message}`, 'error');
            } finally {
                ocrBtn.textContent = 'Run OCR on Current Frame';
                ocrBtn.disabled = false;
            }
        }

        // --- Firestore Session Management ---

        /**
         * Starts a new recording session.
         */
        async function startSession() {
            if (!isAuthReady || !userId || !db) {
                logMessage('Cannot start session: Authentication not ready.', 'error');
                return;
            }
            if (sessionData) {
                logMessage('A session is already active. Please stop the current one first.', 'warning');
                return;
            }

            sessionData = {
                startTime: serverTimestamp(),
                detections: [] // Array to store sampled detection data
            };
            
            logMessage('Session recording started...');
            document.getElementById('startSessionBtn').disabled = true;
            document.getElementById('stopSessionBtn').disabled = false;
        }

        /**
         * Samples and saves a detection event to the current session buffer.
         */
        function saveDetectionToSession(predictions) {
            // Only sample every few frames to prevent hitting Firestore limits
            if (sessionData.detections.length < 500) { // Limit total detections to 500
                const simplifiedDetections = predictions.map(p => ({
                    class: p.class,
                    score: parseFloat(p.score.toFixed(2)),
                    bbox: p.bbox.map(v => Math.round(v))
                }));
                
                if (simplifiedDetections.length > 0) {
                    sessionData.detections.push({
                        time: Date.now(),
                        data: simplifiedDetections
                    });
                }
            }
        }

        /**
         * Stops the current recording session and saves the data to Firestore.
         */
        async function stopAndSaveSession() {
            if (!sessionData) {
                logMessage('No active session to stop.', 'warning');
                return;
            }
            if (!isAuthReady || !userId || !db) {
                logMessage('Cannot save session: Authentication not ready.', 'error');
                return;
            }

            // Finalize session data
            const finalSessionData = {
                ...sessionData,
                endTime: serverTimestamp(),
                totalDetectionsCount: sessionData.detections.length,
                // Serialize the complex detections array to a string to ensure successful storage,
                // as highly nested/large arrays can sometimes cause Firestore issues.
                detections: JSON.stringify(sessionData.detections),
                summary: `Session recorded with ${sessionData.detections.length} events.`
            };
            
            // Clear current session state
            sessionData = null;

            try {
                // Path: /artifacts/{appId}/users/{userId}/sessions/{docId}
                const sessionsRef = collection(db, `artifacts/${appId}/users/${userId}/${SESSIONS_COLLECTION}`);
                await addDoc(sessionsRef, finalSessionData);
                logMessage('Session successfully saved to Firestore!', 'success');
            } catch (error) {
                console.error("Error saving session:", error);
                logMessage(`Error saving session: ${error.message}`, 'error');
            } finally {
                document.getElementById('startSessionBtn').disabled = false;
                document.getElementById('stopSessionBtn').disabled = true;
            }
        }
        
        /**
         * Sets up real-time listener for the user's past sessions.
         */
        function setupSessionHistoryListener() {
            if (!isAuthReady || !userId || !db) return;

            const sessionsRef = collection(db, `artifacts/${appId}/users/${userId}/${SESSIONS_COLLECTION}`);
            const q = query(sessionsRef);
            
            onSnapshot(q, (snapshot) => {
                const historyList = document.getElementById('sessionHistory');
                historyList.innerHTML = '';
                
                if (snapshot.empty) {
                    historyList.innerHTML = '<p class="text-center text-sm p-4 text-gray-500">No previous sessions found.</p>';
                    return;
                }

                snapshot.docs.forEach(doc => {
                    const session = doc.data();
                    const li = document.createElement('li');
                    li.className = 'border-b border-gray-700 p-3 hover:bg-gray-800 transition rounded-md mb-2';
                    
                    const startTime = session.startTime?.toDate().toLocaleTimeString() || 'N/A';
                    const endTime = session.endTime?.toDate().toLocaleTimeString() || 'N/A';
                    const date = session.startTime?.toDate().toLocaleDateString() || 'N/A';

                    // Attempt to show some detail from the detections string
                    const detectionSummary = session.totalDetectionsCount ? 
                        `<span class="text-yellow-400">${session.totalDetectionsCount} events</span>` : 
                        'No events';
                    
                    li.innerHTML = `
                        <div class="font-semibold text-white text-sm">Session ID: ${doc.id.substring(0, 8)}...</div>
                        <div class="text-xs text-gray-400">Date: ${date} (${startTime} - ${endTime})</div>
                        <div class="text-xs text-gray-400">Events: ${detectionSummary}</div>
                    `;
                    historyList.appendChild(li);
                });
                logMessage(`Loaded ${snapshot.size} session records.`, 'info');
            }, (error) => {
                console.error("Error listening to sessions:", error);
                logMessage(`Error loading session history: ${error.message}`, 'error');
            });
        }


        // --- Utility Functions ---

        /**
         * Displays status messages in the log panel.
         */
        function logMessage(message, type = 'info') {
            const logPanel = document.getElementById('logPanel');
            const p = document.createElement('p');
            const time = new Date().toLocaleTimeString();
            
            let color = 'text-gray-400';
            if (type === 'error') color = 'text-red-400';
            else if (type === 'success') color = 'text-green-400';
            else if (type === 'warning') color = 'text-yellow-400';

            p.className = `text-xs ${color} mb-1`;
            p.innerHTML = `[${time}] ${message}`;
            
            logPanel.prepend(p);
            // Limit log entries to keep performance good
            while (logPanel.children.length > 50) {
                logPanel.removeChild(logPanel.lastChild);
            }
        }

        // --- Main Execution on Load ---
        window.onload = function() {
            logMessage('Application started. Loading Firebase and ML models...');
            initializeFirebase();
            setupCameraAndML();

            // Attach event listeners
            document.getElementById('startDetectionBtn').onclick = startDetection;
            document.getElementById('stopDetectionBtn').onclick = stopDetection;
            document.getElementById('runOCRBtn').onclick = runOCR;
            document.getElementById('startSessionBtn').onclick = startSession;
            document.getElementById('stopSessionBtn').onclick = stopAndSaveSession;
        };

    </script>
</head>
<body class="p-4">

    <div class="container-app">
        
        <!-- Main Vision Area -->
        <div class="col-span-1">
            <h1 class="text-3xl font-bold mb-2 text-white">AI Vision Console</h1>
            <p id="userIdDisplay" class="text-sm text-gray-500 mb-4">User ID: ...</p>
            
            <!-- Video & Canvas Container -->
            <div class="video-container mb-6">
                <video id="webcamVideo" autoplay muted playsinline></video>
                <canvas id="predictionCanvas"></canvas>
            </div>

            <!-- Controls Panel -->
            <div class="grid grid-cols-2 gap-4 mb-6">
                <button id="startDetectionBtn" class="control-btn bg-blue-600 hover:bg-blue-700" disabled>
                    Start Detection
                </button>
                <button id="stopDetectionBtn" class="control-btn bg-red-600 hover:bg-red-700" disabled>
                    Stop Detection
                </button>
                <button id="runOCRBtn" class="control-btn bg-purple-600 hover:bg-purple-700">
                    Run OCR on Current Frame
                </button>
                <button id="startSessionBtn" class="control-btn bg-green-600 hover:bg-green-700">
                    Start Session Record
                </button>
                <button id="stopSessionBtn" class="control-btn bg-orange-600 hover:bg-orange-700" disabled>
                    Stop & Save Session
                </button>
            </div>
            
            <!-- Results and OCR Panel (Visible on Mobile) -->
            <div class="bg-gray-800 p-4 rounded-xl shadow-inner md:hidden mb-6">
                <h2 class="text-lg font-semibold border-b border-gray-700 pb-2 mb-2 text-white">Current Detections</h2>
                <ul id="currentDetections" class="space-y-1">
                    <li class="text-sm text-gray-500">Detection not active.</li>
                </ul>
                
                <h2 class="text-lg font-semibold border-b border-gray-700 pt-4 pb-2 mb-2 text-white">OCR Result</h2>
                <div id="ocrResult" class="bg-gray-900 p-3 rounded-md min-h-[50px]">
                    <p class="text-sm text-gray-500">Press 'Run OCR' to recognize text.</p>
                </div>
            </div>
        </div>

        <!-- Sidebar / History Panel -->
        <div class="col-span-1">
            <div class="bg-gray-800 p-4 rounded-xl shadow-lg mb-6 sticky top-4">
                <h2 class="text-xl font-bold mb-4 text-white">Results & History</h2>

                <!-- Current Detections (Hidden on Mobile, shown in main section) -->
                <div class="hidden md:block mb-4">
                    <h3 class="text-lg font-semibold border-b border-gray-700 pb-2 mb-2 text-white">Live Object Counts</h3>
                    <ul id="currentDetections" class="space-y-1">
                        <li class="text-sm text-gray-500">Detection not active.</li>
                    </ul>
                </div>
                
                <!-- OCR Result -->
                <div class="hidden md:block mb-6">
                    <h3 class="text-lg font-semibold border-b border-gray-700 pb-2 mb-2 text-white">OCR Result</h3>
                    <div id="ocrResult" class="bg-gray-900 p-3 rounded-md min-h-[50px]">
                        <p class="text-sm text-gray-500">Press 'Run OCR' to recognize text.</p>
                    </div>
                </div>

                <!-- Session History -->
                <div class="mb-6">
                    <h3 class="text-lg font-semibold border-b border-gray-700 pb-2 mb-2 text-white">Session History (Firestore)</h3>
                    <ul id="sessionHistory" class="space-y-2">
                        <li class="text-sm text-gray-500 text-center p-4">Loading sessions...</li>
                    </ul>
                </div>

                <!-- Log Panel -->
                <div>
                    <h3 class="text-lg font-semibold border-b border-gray-700 pb-2 mb-2 text-white">System Log</h3>
                    <div id="logPanel" class="p-3 rounded-md">
                        <p class="text-xs text-gray-500 mb-1">Waiting for user interaction...</p>
                    </div>
                </div>
            </div>
        </div>

    </div>

</body>
</html>


